{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Agent Plateau Weekly Reports","text":"<p>\u6b22\u8fce\u8bbf\u95ee\u9879\u76ee\u62a5\u544a\u7f51\u7ad9\u3002</p>"},{"location":"#weekly-updates","title":"\ud83d\udcc5 Weekly Updates","text":"<ul> <li>Week 1: 25/11/3</li> </ul>"},{"location":"#project-overview","title":"\ud83e\udde0 Project Overview","text":"<p>\u672c\u9879\u76ee\u65e8\u5728\u7814\u7a76\u81ea\u8fdb\u5316 Agent \u7684 scaling law\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u8fed\u4ee3\u5b9a\u4e49\uff08\u5982 rollouts number \u4e0e FLOPs\uff09\u63a2\u7d22\u7edf\u4e00\u5ea6\u91cf\u6807\u51c6\u3002</p>"},{"location":"week1/","title":"Week 1","text":""},{"location":"week1/#1-background-and-objective","title":"1. Background and Objective","text":"<ul> <li>We aim to compare self-evolving/self-optimizing systems (GEPA, ACE) under different levels of inner-loop search/evaluation intensity, focusing on their performance\u2013cost\u2013compute scaling behavior.</li> <li>The key question: how should we define the horizontal axis for scaling?</li> <li>Initial candidates:</li> <li>Number of rollouts  </li> <li>Number of iterations  </li> <li>Token usage  </li> <li>Total cost (USD)</li> </ul>"},{"location":"week1/#2-current-gepa-experiments-and-key-findings","title":"2. Current GEPA Experiments and Key Findings","text":"<p>Model and Configurations: <code>gpt-4o-mini</code> with configurations \u2014 <code>Baseline</code>, <code>GEPA-5</code>, <code>GEPA-10</code>, <code>GEPA-15</code>, <code>GEPA-20</code></p> <p>Evaluation Datasets: HotpotQA, HoVer (consistent with default script settings)</p> <p></p>"},{"location":"week1/#cost-scaling","title":"Cost Scaling","text":""},{"location":"week1/#performance-cost-tradeoff","title":"Performance-Cost Tradeoff","text":""},{"location":"week1/#conclusions","title":"Conclusions","text":"<ol> <li> <p>Costs remain roughly constant while performance improves with more iterations.</p> </li> <li> <p>GEPA-10/15/20 show similar API spending, but performance continues to increase.</p> </li> <li>The number of external API calls increases, but total output tokens do not rise significantly.</li> <li>Iteration count could be a candidate, but its definition varies across frameworks.</li> <li> <p>The GEPA paper uses rollouts as its metric, but rollout semantics differ by architecture or task.</p> </li> <li> <p>Number of metric evaluations may serve as a better scaling indicator. For example:</p> </li> </ol> <pre><code>def one_metric_call(program, example):\n    output = program(example.question)  # may internally call GPT 2\u20133 times\n    # Step 2: Evaluate the result (possibly another GPT API call)\n    score = metric_fn(example, output)  # may call GPT once more\n    return score  # counted as 1 metric call\n</code></pre>"}]}