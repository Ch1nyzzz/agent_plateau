uv run python -m scripts.run_experiments --bm_idx 0 --benchmark_name "hoverBench" --num_threads 32 --program_idx 0 --prog_name "HoverMultiHop" --opt_idx 0 --optim_name "Baseline" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0
uv run python -m scripts.run_experiments --bm_idx 0 --benchmark_name "hoverBench" --num_threads 32 --program_idx 0 --prog_name "HoverMultiHop" --opt_idx 1 --optim_name "MIPROv2-Heavy" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt Baseline
uv run python -m scripts.run_experiments --bm_idx 0 --benchmark_name "hoverBench" --num_threads 32 --program_idx 0 --prog_name "HoverMultiHop" --opt_idx 2 --optim_name "GEPA-MERGE" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt MIPROv2-Heavy
uv run python -m scripts.run_experiments --bm_idx 0 --benchmark_name "hoverBench" --num_threads 32 --program_idx 0 --prog_name "HoverMultiHop" --opt_idx 3 --optim_name "GEPA" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt MIPROv2-Heavy
uv run python -m scripts.run_experiments --bm_idx 0 --benchmark_name "hoverBench" --num_threads 32 --program_idx 0 --prog_name "HoverMultiHop" --opt_idx 4 --optim_name "Abl-SelectBestCandidate" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt MIPROv2-Heavy
uv run python -m scripts.run_experiments --bm_idx 0 --benchmark_name "hoverBench" --num_threads 32 --program_idx 0 --prog_name "HoverMultiHop" --opt_idx 5 --optim_name "GRPO" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0
uv run python -m scripts.run_experiments --bm_idx 1 --benchmark_name "HotpotQABench" --num_threads 32 --program_idx 0 --prog_name "HotpotMultiHop" --opt_idx 0 --optim_name "Baseline" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0
uv run python -m scripts.run_experiments --bm_idx 1 --benchmark_name "HotpotQABench" --num_threads 32 --program_idx 0 --prog_name "HotpotMultiHop" --opt_idx 1 --optim_name "MIPROv2-Heavy" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt Baseline
uv run python -m scripts.run_experiments --bm_idx 1 --benchmark_name "HotpotQABench" --num_threads 32 --program_idx 0 --prog_name "HotpotMultiHop" --opt_idx 2 --optim_name "GEPA-MERGE" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt MIPROv2-Heavy
uv run python -m scripts.run_experiments --bm_idx 1 --benchmark_name "HotpotQABench" --num_threads 32 --program_idx 0 --prog_name "HotpotMultiHop" --opt_idx 3 --optim_name "GEPA" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt MIPROv2-Heavy
uv run python -m scripts.run_experiments --bm_idx 1 --benchmark_name "HotpotQABench" --num_threads 32 --program_idx 0 --prog_name "HotpotMultiHop" --opt_idx 4 --optim_name "Abl-SelectBestCandidate" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt MIPROv2-Heavy
uv run python -m scripts.run_experiments --bm_idx 1 --benchmark_name "HotpotQABench" --num_threads 32 --program_idx 0 --prog_name "HotpotMultiHop" --opt_idx 5 --optim_name "GRPO" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0
uv run python -m scripts.run_experiments --bm_idx 2 --benchmark_name "Papillon" --num_threads 32 --program_idx 0 --prog_name "PAPILLON" --opt_idx 0 --optim_name "Baseline" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0
uv run python -m scripts.run_experiments --bm_idx 2 --benchmark_name "Papillon" --num_threads 32 --program_idx 0 --prog_name "PAPILLON" --opt_idx 1 --optim_name "MIPROv2-Heavy" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt Baseline
uv run python -m scripts.run_experiments --bm_idx 2 --benchmark_name "Papillon" --num_threads 32 --program_idx 0 --prog_name "PAPILLON" --opt_idx 2 --optim_name "GEPA-MERGE" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt MIPROv2-Heavy
uv run python -m scripts.run_experiments --bm_idx 2 --benchmark_name "Papillon" --num_threads 32 --program_idx 0 --prog_name "PAPILLON" --opt_idx 3 --optim_name "GEPA" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt MIPROv2-Heavy
uv run python -m scripts.run_experiments --bm_idx 2 --benchmark_name "Papillon" --num_threads 32 --program_idx 0 --prog_name "PAPILLON" --opt_idx 4 --optim_name "Abl-SelectBestCandidate" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt MIPROv2-Heavy
uv run python -m scripts.run_experiments --bm_idx 2 --benchmark_name "Papillon" --num_threads 32 --program_idx 0 --prog_name "PAPILLON" --opt_idx 5 --optim_name "GRPO" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0
uv run python -m scripts.run_experiments --bm_idx 3 --benchmark_name "IFBench" --num_threads 32 --program_idx 0 --prog_name "IFBenchCoT2StageProgram" --opt_idx 0 --optim_name "Baseline" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0
uv run python -m scripts.run_experiments --bm_idx 3 --benchmark_name "IFBench" --num_threads 32 --program_idx 0 --prog_name "IFBenchCoT2StageProgram" --opt_idx 1 --optim_name "MIPROv2-Heavy" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt Baseline
uv run python -m scripts.run_experiments --bm_idx 3 --benchmark_name "IFBench" --num_threads 32 --program_idx 0 --prog_name "IFBenchCoT2StageProgram" --opt_idx 2 --optim_name "GEPA-MERGE" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt MIPROv2-Heavy
uv run python -m scripts.run_experiments --bm_idx 3 --benchmark_name "IFBench" --num_threads 32 --program_idx 0 --prog_name "IFBenchCoT2StageProgram" --opt_idx 3 --optim_name "GEPA" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt MIPROv2-Heavy
uv run python -m scripts.run_experiments --bm_idx 3 --benchmark_name "IFBench" --num_threads 32 --program_idx 0 --prog_name "IFBenchCoT2StageProgram" --opt_idx 4 --optim_name "Abl-SelectBestCandidate" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt MIPROv2-Heavy
uv run python -m scripts.run_experiments --bm_idx 3 --benchmark_name "IFBench" --num_threads 32 --program_idx 0 --prog_name "IFBenchCoT2StageProgram" --opt_idx 5 --optim_name "GRPO" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0
uv run python -m scripts.run_experiments --bm_idx 4 --benchmark_name "LiveBenchMathBench" --num_threads 32 --program_idx 0 --prog_name "CoT" --opt_idx 0 --optim_name "Baseline" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0
uv run python -m scripts.run_experiments --bm_idx 4 --benchmark_name "LiveBenchMathBench" --num_threads 32 --program_idx 0 --prog_name "CoT" --opt_idx 1 --optim_name "MIPROv2-Heavy" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt Baseline
uv run python -m scripts.run_experiments --bm_idx 4 --benchmark_name "LiveBenchMathBench" --num_threads 32 --program_idx 0 --prog_name "CoT" --opt_idx 2 --optim_name "GEPA-MERGE" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt MIPROv2-Heavy
uv run python -m scripts.run_experiments --bm_idx 4 --benchmark_name "LiveBenchMathBench" --num_threads 32 --program_idx 0 --prog_name "CoT" --opt_idx 3 --optim_name "GEPA" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt MIPROv2-Heavy
uv run python -m scripts.run_experiments --bm_idx 4 --benchmark_name "LiveBenchMathBench" --num_threads 32 --program_idx 0 --prog_name "CoT" --opt_idx 4 --optim_name "Abl-SelectBestCandidate" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0 --use_cache_from_opt MIPROv2-Heavy
uv run python -m scripts.run_experiments --bm_idx 4 --benchmark_name "LiveBenchMathBench" --num_threads 32 --program_idx 0 --prog_name "CoT" --opt_idx 5 --optim_name "GRPO" --lm_config '{"name": "qwen3-8b", "model": "openai/arbor:qwen/qwen3-8b", "api_key": "API_KEY", "api_base": "http://localhost:{portnum}/v1/", "temperature": 0.6, "top_p": 0.95, "top_k": 20, "launch_kwargs": {"max_context_length": 8192}, "train_kwargs": {"update_interval": 1, "per_device_train_batch_size": 1, "gradient_accumulation_steps": 20, "temperature": 0.6, "beta": 0.01, "learning_rate": 1e-05, "gradient_checkpointing": true, "gradient_checkpointing_kwargs": {"use_reentrant": false}, "bf16": true, "lr_scheduler_type": "constant_with_warmup", "max_prompt_length": null, "max_completion_length": null, "scale_rewards": true, "max_grad_norm": 0.1, "lora": true, "report_to": "wandb", "log_completions": true, "logging_steps": 100, "generation_batch_size": 12}}' --seed 0
