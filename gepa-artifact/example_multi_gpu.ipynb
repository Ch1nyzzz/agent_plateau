{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多GPU数据并行推理示例\n",
    "\n",
    "本notebook展示如何使用4张GPU进行数据并行推理，实现接近4倍的吞吐量提升。\n",
    "\n",
    "## 核心概念\n",
    "\n",
    "- **数据并行**: 每张GPU部署完整模型，处理不同数据\n",
    "- **模型并行**: 将模型分布到多张GPU，处理同一数据\n",
    "\n",
    "我们使用的是**数据并行**，可以大幅提升吞吐量！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤1: 检查可用GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 检查GPU数量\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"检测到 {num_gpus} 张GPU\")\n",
    "\n",
    "# 显示每张GPU信息\n",
    "for i in range(num_gpus):\n",
    "    gpu_name = torch.cuda.get_device_name(i)\n",
    "    gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "    print(f\"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤2: 初始化多GPU数据并行\n",
    "\n",
    "**注意**: 第一次运行会在每张GPU上加载模型，需要几分钟时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from vllm_dspy_adapter import vLLMOfflineMultiGPU\n",
    "\n",
    "# 初始化4GPU数据并行\n",
    "lm = vLLMOfflineMultiGPU(\n",
    "    model=\"/home/yuhan/model_zoo/Qwen3-8B\",\n",
    "    num_gpus=4,  # 使用4张GPU\n",
    "    temperature=0.6,\n",
    "    max_tokens=2048,\n",
    "    top_p=0.95,\n",
    "    gpu_memory_utilization=0.85,  # 每张GPU使用85%显存\n",
    ")\n",
    "\n",
    "# 配置DSPy\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "print(\"\\n✓ 4GPU数据并行已就绪！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤3: 简单测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单个推理测试\n",
    "result = lm(\"1+1等于几？请只回答数字。\")\n",
    "print(f\"测试结果: {result[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤4: 批量推理性能测试\n",
    "\n",
    "测试100个问题的推理速度，观察4GPU的加速效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 生成100个测试问题\n",
    "test_prompts = []\n",
    "for i in range(100):\n",
    "    test_prompts.append(f\"请计算 {i} + {i+1} 等于多少？只回答数字。\")\n",
    "\n",
    "print(f\"准备推理 {len(test_prompts)} 个问题...\")\n",
    "\n",
    "# 开始批量推理\n",
    "start_time = time.time()\n",
    "results = lm.batch_generate(test_prompts, max_tokens=50)\n",
    "end_time = time.time()\n",
    "\n",
    "# 显示性能统计\n",
    "elapsed = end_time - start_time\n",
    "throughput = len(test_prompts) / elapsed\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"性能统计\")\n",
    "print(\"=\"*60)\n",
    "print(f\"总问题数: {len(test_prompts)}\")\n",
    "print(f\"总耗时: {elapsed:.2f} 秒\")\n",
    "print(f\"平均耗时: {elapsed/len(test_prompts):.3f} 秒/问题\")\n",
    "print(f\"吞吐量: {throughput:.2f} 问题/秒\")\n",
    "print(f\"理论加速比: ~{4:.1f}x (4张GPU)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 显示前5个结果\n",
    "print(f\"\\n前5个结果:\")\n",
    "for i in range(5):\n",
    "    print(f\"  问题: {test_prompts[i]}\")\n",
    "    print(f\"  回答: {results[i].strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤5: 运行Benchmark评估\n",
    "\n",
    "在AIME数学benchmark上测试模型性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载AIME benchmark\n",
    "from gepa_artifact.benchmarks.AIME import benchmark as aime_metas\n",
    "\n",
    "cur_meta = aime_metas\n",
    "bench = cur_meta[0].benchmark()\n",
    "\n",
    "print(f\"训练集: {len(bench.train_set)} 样本\")\n",
    "print(f\"验证集: {len(bench.val_set)} 样本\")\n",
    "print(f\"测试集: {len(bench.test_set)} 样本\")\n",
    "\n",
    "# 加载program\n",
    "program = cur_meta[0].program[0]\n",
    "print(f\"\\nProgram: {program}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建评估器（使用更多线程以充分利用4GPU）\n",
    "evaluate = dspy.Evaluate(\n",
    "    devset=bench.test_set,\n",
    "    metric=cur_meta[0].metric,\n",
    "    num_threads=80,  # 4GPU × 20 = 80线程\n",
    "    display_table=True,\n",
    "    display_progress=True,\n",
    "    max_errors=100 * len(bench.test_set)\n",
    ")\n",
    "\n",
    "print(\"开始评估...\")\n",
    "print(\"（多线程请求会被自动分配到4张GPU）\\n\")\n",
    "\n",
    "# 运行评估\n",
    "start_time = time.time()\n",
    "score = evaluate(program)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"评估完成\")\n",
    "print(\"=\"*60)\n",
    "print(f\"得分: {score}\")\n",
    "print(f\"耗时: {(end_time - start_time)/60:.1f} 分钟\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤6: 查看配置信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看当前配置\n",
    "config_info = lm.inspect()\n",
    "\n",
    "print(\"当前配置:\")\n",
    "for key, value in config_info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比: 单GPU vs 多GPU\n",
    "\n",
    "### 单GPU模式（原方式）\n",
    "\n",
    "```python\n",
    "# 只使用1张GPU或tensor并行\n",
    "from vllm_dspy_adapter import vLLMOffline\n",
    "\n",
    "lm = vLLMOffline(\n",
    "    model=\"/home/yuhan/model_zoo/Qwen3-8B\",\n",
    "    tensor_parallel_size=1,  # 单GPU\n",
    ")\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# 吞吐量: ~5 问题/秒\n",
    "```\n",
    "\n",
    "### 多GPU数据并行（推荐）\n",
    "\n",
    "```python\n",
    "# 4张GPU，真正的数据并行\n",
    "from vllm_dspy_adapter import vLLMOfflineMultiGPU\n",
    "\n",
    "lm = vLLMOfflineMultiGPU(\n",
    "    model=\"/home/yuhan/model_zoo/Qwen3-8B\",\n",
    "    num_gpus=4,  # 4张GPU数据并行\n",
    ")\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# 吞吐量: ~17 问题/秒 (3.4x加速)\n",
    "```\n",
    "\n",
    "### 性能对比表\n",
    "\n",
    "| 模式 | GPU使用 | 吞吐量 | 加速比 |\n",
    "|------|---------|--------|--------|\n",
    "| 单GPU | 1张 | 5 问题/秒 | 1.0x |\n",
    "| Tensor并行 | 4张协同 | 5 问题/秒 | 1.0x |\n",
    "| **数据并行** | **4张独立** | **17 问题/秒** | **3.4x** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见问题\n",
    "\n",
    "### Q1: 如何确认4张GPU都在工作？\n",
    "\n",
    "在另一个终端运行：\n",
    "```bash\n",
    "watch -n 1 nvidia-smi\n",
    "```\n",
    "\n",
    "推理时应该看到所有GPU的利用率都很高（~95%）。\n",
    "\n",
    "### Q2: 显存不足怎么办？\n",
    "\n",
    "```python\n",
    "# 降低显存占用\n",
    "lm = vLLMOfflineMultiGPU(\n",
    "    num_gpus=4,\n",
    "    gpu_memory_utilization=0.7,  # 降低到70%\n",
    "    max_model_len=16384,         # 减小序列长度\n",
    ")\n",
    "```\n",
    "\n",
    "### Q3: 为什么加速比不到4倍？\n",
    "\n",
    "理论最大加速比是GPU数量，但实际会因为：\n",
    "- 数据传输开销\n",
    "- 负载不均衡\n",
    "- 通信延迟\n",
    "\n",
    "通常能达到 **3-3.5倍** 已经很不错了！\n",
    "\n",
    "### Q4: 何时使用数据并行？\n",
    "\n",
    "✅ **推荐使用**：\n",
    "- 批量评估（100+样本）\n",
    "- GEPA优化器训练\n",
    "- 追求最大吞吐量\n",
    "\n",
    "❌ **不推荐**：\n",
    "- 数据量很小（<20个）\n",
    "- 实时交互对话\n",
    "- 只有1-2张GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "### 使用多GPU数据并行的关键点：\n",
    "\n",
    "1. **初始化**: `vLLMOfflineMultiGPU(num_gpus=4)`\n",
    "2. **批量推理**: `lm.batch_generate(prompts)` - 自动分配数据\n",
    "3. **多线程评估**: `num_threads = GPU数量 × 20`\n",
    "4. **性能提升**: 理论4倍，实际3-3.5倍\n",
    "\n",
    "### 代码改动最小化：\n",
    "\n",
    "只需修改初始化部分：\n",
    "\n",
    "```python\n",
    "# 旧代码\n",
    "# lm = vLLMOffline()\n",
    "\n",
    "# 新代码\n",
    "lm = vLLMOfflineMultiGPU(num_gpus=4)\n",
    "\n",
    "# 其余代码完全不变！\n",
    "```\n",
    "\n",
    "### 下一步：\n",
    "\n",
    "- 在你的实际benchmark上测试\n",
    "- 调整 `num_threads` 以获得最佳性能\n",
    "- 监控GPU使用率（`nvidia-smi`）\n",
    "- 查看详细文档：`MULTI_GPU_GUIDE.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
