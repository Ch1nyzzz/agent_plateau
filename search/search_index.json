{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Agent Plateau Weekly Reports","text":"<p>Welcome to the report web.</p>"},{"location":"#weekly-updates","title":"\ud83d\udcc5 Weekly Updates","text":"<ul> <li>Week 1: 25/11/3</li> </ul>"},{"location":"#project-overview","title":"\ud83e\udde0 Project Overview","text":"<p>This project aims at researching on the scaling law/plateau of self-evolving Agent\uff0cseeking for unified metrics for scalin.</p>"},{"location":"week1/","title":"Week 1","text":""},{"location":"week1/#1-background-and-objective","title":"1. Background and Objective","text":"<ul> <li>We aim to compare self-evolving/self-optimizing systems (GEPA, ACE) under different levels of inner-loop search/evaluation intensity, focusing on their performance\u2013cost\u2013compute scaling behavior.</li> <li>The key question: how should we define the horizontal axis for scaling?</li> <li>Initial candidates:</li> <li>Number of rollouts  </li> <li>Number of iterations  </li> <li>Token usage  </li> <li>Total cost (USD)</li> </ul>"},{"location":"week1/#2-current-gepa-experiments-and-key-findings","title":"2. Current GEPA Experiments and Key Findings","text":"<p>Model and Configurations: <code>gpt-4o-mini</code> with configurations \u2014 <code>Baseline</code>, <code>GEPA-5</code>, <code>GEPA-10</code>, <code>GEPA-15</code>, <code>GEPA-20</code></p> <p>Evaluation Datasets: HotpotQA, HoVer (consistent with default script settings)</p> <p></p>"},{"location":"week1/#cost-scaling","title":"Cost Scaling","text":""},{"location":"week1/#performance-cost-tradeoff","title":"Performance-Cost Tradeoff","text":""},{"location":"week1/#conclusions","title":"Conclusions","text":"<ol> <li> <p>Costs remain roughly constant while performance improves with more iterations.</p> </li> <li> <p>GEPA-10/15/20 show similar API spending, but performance continues to increase.</p> </li> <li>The number of external API calls increases, but total output tokens do not rise significantly.</li> <li>Iteration count could be a candidate, but its definition varies across frameworks.</li> <li> <p>The GEPA paper uses rollouts as its metric, but rollout semantics differ by architecture or task.</p> </li> <li> <p>Number of metric evaluations may serve as a better scaling indicator. For example:</p> </li> </ol> <pre><code>def one_metric_call(program, example):\n    output = program(example.question)  # may internally call GPT 2\u20133 times\n    # Step 2: Evaluate the result (possibly another GPT API call)\n    score = metric_fn(example, output)  # may call GPT once more\n    return score  # counted as 1 metric call\n</code></pre>"},{"location":"week1/#3-current-issue","title":"3. Current Issue","text":"<p>At present, I\u2019m encountering a resource access issue on the Helios4 cluster. It appears that the nodes are continuously occupied by other lab members, making it extremely difficult to find an available time slot for running experiments. Currently, the experiments on the two benchmarks are being conducted using my personal OpenAI API key, but this setup is not sustainable for continued large-scale experimentation.</p> <p>Therefore, I would like to ask:</p> <ul> <li>Whether it's possible to grant access to additional compute nodes, or</li> <li>If there exists a dashboard or monitoring tool to better visualize GPU availability and idle times.</li> </ul> <p>Adam kindly provided me with access credentials for Helios3, but for some reason, I still haven\u2019t been able to log in successfully. He might be quite busy these days, so the issue hasn\u2019t been resolved yet.</p>"}]}